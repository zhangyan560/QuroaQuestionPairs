{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 448,
   "id": "reflected-worthy",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/zhangyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torchtext.legacy.data import Field,BucketIterator,TabularDataset\n",
    "from torchtext.legacy import data\n",
    "import torchtext\n",
    "import spacy\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from tqdm import tqdm\n",
    "from torchtext.vocab import Vectors, GloVe, FastText\n",
    "\n",
    "nltk.download('punkt')\n",
    "%matplotlib inline\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "pal = sns.color_palette()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "sorted-possibility",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the length of train set 404290\n",
      "the length of test set 2345796\n"
     ]
    }
   ],
   "source": [
    "data_train = pd.read_csv('train.csv')\n",
    "data_test = pd.read_csv(\"test.csv\")\n",
    "print(\"the length of train set\",len(data_train))\n",
    "print(\"the length of test set\", len(data_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "intensive-sherman",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data_train[:50000]\n",
    "validation = data_train[50000:52000]\n",
    "test = data_test\n",
    "# [i for i in data_train.question1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "chicken-mention",
   "metadata": {},
   "outputs": [],
   "source": [
    "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\",'i\\'m':'i am', \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\", 'colour': 'color', 'centre': 'center', 'favourite': 'favorite', 'travelling': 'traveling', 'counselling': 'counseling', 'theatre': 'theater', 'cancelled': 'canceled'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "banned-institute",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_contractions(text, mapping):\n",
    "    text = text.lower()\n",
    "    specials = [\"’\", \"‘\", \"´\", \"`\"]\n",
    "    for s in specials:\n",
    "        text = text.replace(s, \"'\")\n",
    "    text = ' '.join([mapping[t] if t in mapping else mapping[t.lower()] if t.lower() in mapping else t for t in text.split(\" \")])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "selected-aaron",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_newlines(sent):\n",
    "    sent = re.sub(r'\\s+', \" \", sent )\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "stable-terrorist",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhangyan/anaconda/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/Users/zhangyan/anaconda/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/Users/zhangyan/anaconda/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/zhangyan/anaconda/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n",
      "/Users/zhangyan/anaconda/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/Users/zhangyan/anaconda/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  import sys\n",
      "/Users/zhangyan/anaconda/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/Users/zhangyan/anaconda/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "train['question1'] = train['question1'].apply(lambda x: clean_contractions(str(x),contraction_mapping))\n",
    "train['question2'] = train['question2'].apply(lambda x: clean_contractions(str(x),contraction_mapping))\n",
    "validation['question1'] = validation['question1'].apply(lambda x: clean_contractions(str(x),contraction_mapping))\n",
    "validation['question2'] = validation['question2'].apply(lambda x: clean_contractions(str(x),contraction_mapping))\n",
    "\n",
    "train['question1'] = train['question1'].apply(lambda x: remove_newlines(str(x)))\n",
    "train['question2'] = train['question2'].apply(lambda x: remove_newlines(str(x)))\n",
    "validation['question1'] = validation['question1'].apply(lambda x: remove_newlines(str(x)))\n",
    "validation['question2'] = validation['question2'].apply(lambda x: remove_newlines(str(x)))\n",
    "test['question1'] = test['question1'].apply(lambda x: clean_contractions(str(x),contraction_mapping))\n",
    "test['question2'] = test['question2'].apply(lambda x: clean_contractions(str(x),contraction_mapping))\n",
    "\n",
    "test['question1'] = test['question1'].apply(lambda x: remove_newlines(str(x)))\n",
    "test['question2'] = test['question2'].apply(lambda x: remove_newlines(str(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "banned-cause",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = train['is_duplicate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "occasional-ethics",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhangyan/anaconda/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:4315: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['test_id'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-386-6a62094e064f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'qid1'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'qid2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'qid1'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'qid2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4313\u001b[0m             \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4314\u001b[0m             \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4315\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4316\u001b[0m         )\n\u001b[1;32m   4317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4151\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4152\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4153\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4155\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[0;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[1;32m   4186\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4187\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4188\u001b[0;31m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4189\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   5589\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5590\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5591\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{labels[mask]} not found in axis\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5592\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5593\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['test_id'] not found in axis\""
     ]
    }
   ],
   "source": [
    "validation.drop(['id','qid1','qid2'],inplace=True,axis = 1)\n",
    "train.drop(['id','qid1','qid2'],inplace=True,axis = 1)\n",
    "test.drop(['test_id'],inplace=True,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "flying-thong",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('data/Train.csv',index = False)\n",
    "validation.to_csv('data/Validation.csv',index = False)\n",
    "test.to_csv('data/Test.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "chronic-integral",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT1 = data.Field(tokenize='spacy', batch_first=True, lower=True, include_lengths=True)\n",
    "TEXT2 = data.Field(tokenize='spacy', batch_first=True, lower=True, include_lengths=True)\n",
    "LABEL = data.LabelField(dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "original-award",
   "metadata": {},
   "outputs": [],
   "source": [
    "fields1 = [('question1',TEXT1),('question2',TEXT2),('is_duplicate',LABEL)]\n",
    "fields2 = [('question1',TEXT1),('question2',TEXT2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "alleged-paper",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data = data.TabularDataset.splits(\n",
    "    path='data/',\n",
    "    train='Train.csv',\n",
    "    validation = 'Validation.csv',\n",
    "    format='csv',\n",
    "    fields=fields1,\n",
    "    skip_header=True\n",
    ")\n",
    "\n",
    "test_data = data.TabularDataset(\n",
    "        path='data/Test.csv', format='csv',\n",
    "        skip_header=True,\n",
    "        fields=fields2\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "id": "central-allen",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT1.build_vocab(\n",
    "        train_data, val_data, test_data, max_size = 25000  # We use it for getting vocabulary of words\n",
    "    )\n",
    "TEXT2.build_vocab(\n",
    "        train_data, val_data, test_data, max_size = 25000   # We use it for getting vocabulary of words\n",
    "    )\n",
    "LABEL.build_vocab(train_data,val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "id": "macro-skill",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_embeddings = TEXT1.vocab.vectors\n",
    "# vocab_size = len(TEXT1.vocab)\n",
    "\n",
    "def get_iterator(dataset, batch_size, train=True,\n",
    "                 shuffle=True, repeat=False):\n",
    "    \n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available()\n",
    "                          else 'cpu')\n",
    "    \n",
    "    dataset_iter = data.Iterator(\n",
    "        dataset, batch_size=batch_size, device=device,\n",
    "        train=train, shuffle=shuffle, repeat=repeat,\n",
    "        sort=False\n",
    "    )\n",
    "    \n",
    "    return dataset_iter\n",
    "batchsize = 64\n",
    "\n",
    "train_iter = get_iterator(train_data, batch_size=batchsize, \n",
    "                              train=True, shuffle=True,\n",
    "                              repeat=False)\n",
    "val_iter = get_iterator(val_data, batch_size=batchsize, \n",
    "                        train=True, shuffle=True,\n",
    "                        repeat=False)\n",
    "test_iter = get_iterator(test_data, batch_size=batchsize, \n",
    "                             train=False, shuffle=False,\n",
    "                             repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "id": "regular-christmas",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25002\n",
      "25002\n"
     ]
    }
   ],
   "source": [
    "print(len(TEXT1.vocab))\n",
    "print(len(TEXT2.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "id": "israeli-deficit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('?', 2520795), ('the', 1027345), ('what', 943820), ('is', 832413), ('i', 644402), ('how', 633806), ('a', 608286), ('in', 596840), ('to', 569904), ('do', 451711), ('of', 445111), ('are', 398318), ('and', 372856), ('can', 335235), ('for', 314164), (',', 293943), ('why', 235395), ('you', 231209), ('it', 203235), ('my', 198991)]\n"
     ]
    }
   ],
   "source": [
    "print(TEXT1.vocab.freqs.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bizarre-rolling",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "id": "eastern-contribution",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, batch_size, bidirectional, num_layers, vocab_size, embedding_dim, hidden_dim,linear_dim2,linear_dim3, output_dim):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.bidirectional = bidirectional\n",
    "        self.hidden_size = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding(vocab_size,embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first = True, dropout=0.3, bidirectional = self.bidirectional, num_layers = self.num_layers)\n",
    "        self.num_directions  = (1 if self.bidirectional == False else 2)\n",
    "        self.concat_layer = nn.Linear(2*self.num_directions*self.hidden_size, linear_dim2) #for concatenating both the outputs\n",
    "        self.bn3 = nn.BatchNorm1d(linear_dim2,eps=1e-05, momentum=0.1, affine=True)\n",
    "        self.fc1 = nn.Linear(linear_dim2, linear_dim3)#for prediction\n",
    "        self.fc2 = nn.Linear(linear_dim3, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, text1, text2, question1_length, question2_length):\n",
    "        embedded1 = self.embedding(text1)\n",
    "#         embedded1 = nn.utils.rnn.pack_padded_sequence(embedded1.cpu(), question1_length)\n",
    "        embedded2 = self.embedding(text2)\n",
    "#         embedded2 = nn.utils.rnn.pack_padded_sequence(embedded2.cpu(), question2_length)\n",
    "        output_1,(h_n_1,c_n_1) = self.lstm(embedded1)\n",
    "        output_2,(h_n_2,c_n_2) = self.lstm(embedded2)\n",
    "        \n",
    "        output_1 = output_1[:,-1,:].view(self.batch_size,1, self.num_directions*self.hidden_size)\n",
    "        output_2 = output_2[:,-1,:].view(self.batch_size,1,self.num_directions*self.hidden_size)\n",
    "        concatenated_outputs = torch.cat((output_1,output_2),dim = 2)\n",
    "        concatenated_outputs = concatenated_outputs.view(concatenated_outputs.shape[0],concatenated_outputs.shape[2]) \n",
    "        if model.train():\n",
    "            concatenated_outputs = nn.Dropout(p=0.3)(concatenated_outputs)\n",
    "#         concatenated_outputs = self.relu(concatenated_outputs)\n",
    "        concatenated_outputs = self.concat_layer(concatenated_outputs)\n",
    "#         concatenated_outputs = self.relu(concatenated_outputs)\n",
    "        if model.train():\n",
    "            concatenated_outputs = nn.Dropout(p=0.3)(concatenated_outputs)\n",
    "        concatenated_outputs = self.fc1(concatenated_outputs)\n",
    "#         concatenated_outputs = self.relu(concatenated_outputs)\n",
    "        if model.train():\n",
    "            concatenated_outputs = nn.Dropout(p=0.3)(concatenated_outputs)\n",
    "        predictions = self.fc2(concatenated_outputs)\n",
    "        return predictions\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "id": "informative-ranch",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(batch_size = batchsize,\n",
    "             bidirectional = True,\n",
    "             num_layers = 1,\n",
    "             vocab_size = len(TEXT1.vocab),\n",
    "             embedding_dim = 100,\n",
    "             hidden_dim = 50,\n",
    "             linear_dim2 = 32,\n",
    "             linear_dim3 = 16,\n",
    "             output_dim = 1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "id": "funded-spokesman",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr = learning_rate) \n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "id": "premium-smile",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_accuracy(logits,label):\n",
    "    sigmoid = nn.Sigmoid()(logits)\n",
    "    predictions = torch.round(sigmoid)\n",
    "    predictions = predictions.view(batchsize)\n",
    "    return (predictions == label).sum().float()/float(label.size(0))\n",
    "\n",
    "def train(epochs,criterion,optimizer,model,train_iterator,valid_iterator):\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(\"Epoch {} out of {}\".format(epoch,epochs))\n",
    "        \n",
    "        epoch_train_loss = 0\n",
    "        epoch_train_accuracy = 0\n",
    "        \n",
    "        epoch_valid_loss = 0\n",
    "        epoch_valid_accuracy = 0\n",
    "        model.train()\n",
    "        for batch in tqdm(train_iterator):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            question1 = batch.question1[0]\n",
    "            question2 = batch.question2[0]\n",
    "            label = batch.is_duplicate\n",
    "            \n",
    "            question1_length = batch.question1[1]\n",
    "            question2_length = batch.question2[1]\n",
    "            if len(batch.question1[1]) != batchsize:continue\n",
    "            question1.to(device)\n",
    "            question2.to(device)\n",
    "            label.to(device)\n",
    "\n",
    "            label = torch.tensor(label,dtype= torch.float32,device = device)\n",
    "            \n",
    "            predictions = model(question1,question2,question1_length,question2_length)\n",
    "            loss = criterion(predictions,label.unsqueeze(1))\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            batch_loss = loss.item()/len(batch)\n",
    "            batch_accuracy = return_accuracy(predictions,label)\n",
    "            \n",
    "            epoch_train_loss += loss.item()\n",
    "            epoch_train_accuracy += batch_accuracy.item()\n",
    "            \n",
    "        print(\"Epoch Train Accuracy: \",epoch_train_accuracy/len(train_iterator))\n",
    "        print(\"Epoch Train Loss: \",epoch_train_loss/len(train_iterator))\n",
    "        model.eval()\n",
    "        for batch in tqdm(valid_iterator):\n",
    "            model.eval()\n",
    "            question1_v = batch.question1[0]\n",
    "            question2_v = batch.question2[0]\n",
    "            label_v = batch.is_duplicate\n",
    "            \n",
    "            question1_length_v = batch.question1[1]\n",
    "            question2_length_v = batch.question2[1]\n",
    "            if len(question1_length_v) != batchsize:continue\n",
    "            question1_v.to(device)\n",
    "            question2_v.to(device)\n",
    "            label_v.to(device)\n",
    "\n",
    "            label_v = torch.tensor(label_v,dtype= torch.float32,device = device)\n",
    "            \n",
    "            predictions_v = model(question1_v,question2_v,question1_length_v,question2_length_v)\n",
    "            loss_v = criterion(predictions_v,label_v.unsqueeze(1))\n",
    "            \n",
    "            batch_loss_v = loss_v.item()/len(batch)\n",
    "            batch_accuracy_v = return_accuracy(predictions_v,label_v)\n",
    "            \n",
    "            epoch_valid_loss += loss_v.item()\n",
    "            epoch_valid_accuracy += batch_accuracy_v.item()\n",
    "            \n",
    "        print(\"Epoch valid Accuracy: \",epoch_valid_accuracy/len(valid_iterator))\n",
    "        print(\"Epoch valid Loss: \",epoch_valid_loss/len(valid_iterator))\n",
    "        print(\"--\"*60) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "id": "alive-surrey",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/782 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhangyan/anaconda/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "100%|██████████| 782/782 [01:33<00:00,  8.35it/s]\n",
      "  0%|          | 0/32 [00:00<?, ?it/s]/Users/zhangyan/anaconda/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  9%|▉         | 3/32 [00:00<00:01, 22.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Train Accuracy:  0.6650415601023018\n",
      "Epoch Train Loss:  0.6161656396849381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:01<00:00, 21.44it/s]\n",
      "  0%|          | 1/782 [00:00<02:33,  5.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch valid Accuracy:  0.673828125\n",
      "Epoch valid Loss:  0.5560041256248951\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch 1 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [01:34<00:00,  8.29it/s]\n",
      "  9%|▉         | 3/32 [00:00<00:01, 26.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Train Accuracy:  0.7186700767263428\n",
      "Epoch Train Loss:  0.5594128581416576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:01<00:00, 21.75it/s]\n",
      "  0%|          | 1/782 [00:00<02:10,  6.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch valid Accuracy:  0.66748046875\n",
      "Epoch valid Loss:  0.5620379541069269\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch 2 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [01:31<00:00,  8.56it/s]\n",
      "  9%|▉         | 3/32 [00:00<00:01, 21.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Train Accuracy:  0.7540760869565217\n",
      "Epoch Train Loss:  0.5176665574464652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:01<00:00, 21.59it/s]\n",
      "  0%|          | 0/782 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch valid Accuracy:  0.6962890625\n",
      "Epoch valid Loss:  0.621441550552845\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch 3 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [01:32<00:00,  8.47it/s]\n",
      "  9%|▉         | 3/32 [00:00<00:01, 23.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Train Accuracy:  0.7695612212276215\n",
      "Epoch Train Loss:  0.4958963760024751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:01<00:00, 21.90it/s]\n",
      "  0%|          | 0/782 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch valid Accuracy:  0.69580078125\n",
      "Epoch valid Loss:  0.546176003292203\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch 4 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [01:37<00:00,  8.03it/s]\n",
      "  6%|▋         | 2/32 [00:00<00:01, 17.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Train Accuracy:  0.7808503836317136\n",
      "Epoch Train Loss:  0.47271703823905464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:01<00:00, 18.82it/s]\n",
      "  0%|          | 1/782 [00:00<02:14,  5.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch valid Accuracy:  0.6953125\n",
      "Epoch valid Loss:  0.5594775248318911\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch 5 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [01:35<00:00,  8.19it/s]\n",
      "  9%|▉         | 3/32 [00:00<00:01, 23.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Train Accuracy:  0.7955362851662404\n",
      "Epoch Train Loss:  0.4511377202046802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:01<00:00, 21.95it/s]\n",
      "  0%|          | 1/782 [00:00<02:14,  5.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch valid Accuracy:  0.68701171875\n",
      "Epoch valid Loss:  0.5826904037967324\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch 6 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [02:07<00:00,  6.12it/s]\n",
      "  0%|          | 0/32 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Train Accuracy:  0.801770300511509\n",
      "Epoch Train Loss:  0.4376007978568601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:02<00:00, 12.98it/s]\n",
      "  0%|          | 0/782 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch valid Accuracy:  0.68994140625\n",
      "Epoch valid Loss:  0.5586877772584558\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch 7 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [01:41<00:00,  7.73it/s]\n",
      "  9%|▉         | 3/32 [00:00<00:01, 24.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Train Accuracy:  0.8068853900255755\n",
      "Epoch Train Loss:  0.43158045102415793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:01<00:00, 21.39it/s]\n",
      "  0%|          | 1/782 [00:00<02:03,  6.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch valid Accuracy:  0.68603515625\n",
      "Epoch valid Loss:  0.5732296807691455\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch 8 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 216/782 [00:26<01:09,  8.09it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-465-36591abaf055>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-464-9457a4a52fb6>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epochs, criterion, optimizer, model, train_iterator, valid_iterator)\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(epochs,criterion,optimizer,model,train_iter,val_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wireless-criticism",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict(test_iterator,model):\n",
    "    preds = torch.tensor([])\n",
    "    epoch_test_loss = 0\n",
    "    epoch_test_accuracy = 0\n",
    "    for batch in tqdm(test_iterator):\n",
    "        model.eval()\n",
    "        question1 = batch.question1[0]\n",
    "        question2 = batch.question2[0]\n",
    "        question1_length = batch.question1[1]\n",
    "        question2_length = batch.question2[1]\n",
    "\n",
    "        question1.to(device)\n",
    "        question2.to(device)\n",
    "        predictions = model(question1,question2,question1_length,question2_length)\n",
    "        sigmoid = nn.Sigmoid()(predictions)\n",
    "        predictions = torch.round(sigmoid)\n",
    "        predictions = predictions.view(batchsize)\n",
    "        preds = torch.cat((preds, predictions), axis=0)\n",
    "\n",
    "    return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "differential-buffalo",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "preds = predict(test_iter, model)\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "searching-capability",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "personal-freeze",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fixed-circle",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valid-turning",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifth-removal",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sought-display",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
